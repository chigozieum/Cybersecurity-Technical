Comprehensive Guide to Linux System Observability: Mastering the "Big 4" Resources in Enterprise Environments
Introduction to System Observability
In enterprise Linux environments, effective system monitoring is crucial for maintaining optimal performance, preventing outages, and quickly resolving issues when they arise. This comprehensive guide focuses on the "Big 4" resources that every Linux engineer must monitor: CPU, Memory, Disk, and Network. We'll explore a systematic approach to observability through three critical dimensions: utilization, saturation, and errors.

For beginners entering the enterprise Linux space, understanding these monitoring concepts and tools will provide a solid foundation for effective system administration. This guide will walk you through practical commands, explain their output, and provide real-world scenarios where these tools prove invaluable.

Table of Contents
Understanding the Observability Framework
Setting Up Your Monitoring Environment
CPU Monitoring
Memory Monitoring
Disk Monitoring
Network Monitoring
Integrated Monitoring Approaches
Automating Monitoring Tasks
Implementing Alerting Systems
Building Custom Dashboards
Enterprise-Scale Considerations
Troubleshooting Common Issues
Best Practices
Advanced Topics
Conclusion
Understanding the Observability Framework
The "Big 4" resources (CPU, Memory, Disk, and Network) form the foundation of system performance. For each resource, we examine three critical dimensions:

Utilization: How much of the resource is being used
Saturation: Whether the resource is overloaded, causing queuing or delays
Errors: Problems occurring with the resource
This USE (Utilization, Saturation, Errors) methodology, developed by performance expert Brendan Gregg, provides a systematic approach to identifying performance bottlenecks.

Setting Up Your Monitoring Environment
Before diving into specific monitoring tools, let's ensure your environment is properly configured.

Installing Essential Monitoring Packages
# On Debian/Ubuntu systems
sudo apt update
sudo apt install sysstat procps net-tools iproute2 smartmontools dstat htop atop iotop iftop

# On RHEL/CentOS systems
sudo yum install epel-release
sudo yum update
sudo yum install sysstat procps-ng net-tools iproute smartmontools dstat htop atop iotop iftop
Enabling the sysstat Service
The sysstat package provides essential tools like sar, iostat, and mpstat. To ensure it collects data regularly:

# Enable and start the sysstat service
sudo systemctl enable sysstat
sudo systemctl start sysstat

# Verify it's running
sudo systemctl status sysstat
Configuring Collection Intervals
Edit the sysstat configuration to adjust data collection frequency:

# Edit the sysstat configuration file
sudo vi /etc/sysstat/sysstat

# Set collection interval (in seconds)
# Find and modify the INTERVAL parameter
INTERVAL=300  # 5-minute intervals

# Save and restart the service
sudo systemctl restart sysstat
CPU Monitoring
CPU Utilization
CPU utilization tells you how busy your processors are. High utilization isn't necessarily bad, but sustained high utilization can indicate the need for optimization or additional resources.

Using mpstat
The mpstat command provides detailed CPU statistics:

# Display CPU statistics for all CPUs
mpstat -P ALL 1

# Sample output:
# Linux 5.4.0-104-generic (hostname)    05/05/2023    _x86_64_    (8 CPU)
# 
# 01:23:45 PM  CPU    %usr   %nice    %sys %iowait    %irq   %soft  %steal  %guest  %gnice   %idle
# 01:23:46 PM  all   15.63    0.00    5.25    0.13    0.00    0.13    0.00    0.00    0.00   78.87
# 01:23:46 PM    0   18.00    0.00    7.00    0.00    0.00    0.00    0.00    0.00    0.00   75.00
# ...
Key metrics to observe:

%usr: User-level processing
%sys: System-level (kernel) processing
%iowait: Time waiting for I/O operations
%idle: CPU idle time
Using pidstat
To identify which processes are consuming CPU resources:

# Display CPU statistics for processes, refreshing every 2 seconds
pidstat 2

# Monitor a specific process
pidstat -p <PID> 2

# Show top CPU-consuming processes with thread details
pidstat -t -p ALL 1 | sort -nr -k8
CPU Saturation
CPU saturation occurs when processes are waiting for CPU time. This is measured by the run queue length and load average.

Using vmstat
# Display virtual memory statistics, refreshing every 2 seconds
vmstat 2

# Sample output:
# procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----
#  r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
#  2  0      0 7409340 491260 7176292    0    0     0     1    1    2  1  1 98  0  0
#  5  0      0 7409156 491260 7176292    0    0     0    26 1149  994  2  1 97  0  0
The r column shows the run queue length - the number of processes waiting for CPU time. Values consistently above the number of CPU cores indicate saturation.

Using top or uptime for Load Average
# Display system load averages
uptime

# Sample output:
# 13:45:02 up 15 days,  4:34,  3 users,  load average: 1.25, 1.15, 0.99
Load averages represent the average number of processes that are either running or waiting for resources over 1, 5, and 15-minute intervals. As a rule of thumb, sustained load averages higher than the number of CPU cores indicate saturation.

CPU Errors
CPU errors can be hardware-related or manifest as application crashes.

# Check system logs for CPU-related errors
sudo journalctl | grep -i "cpu\|core\|mce"

# Check for hardware errors in the kernel ring buffer
sudo dmesg | grep -i "cpu\|core\|mce"

# Monitor Machine Check Exceptions (hardware errors)
sudo mcelog --daemon --logfile /var/log/mcelog
For more detailed CPU error analysis:

# Install and use lm-sensors for CPU temperature monitoring
sudo apt install lm-sensors
sudo sensors-detect  # Configure sensor detection
sensors  # Display temperatures and other sensor data
Memory Monitoring
Memory Utilization
Memory utilization shows how much RAM is being used by applications, the kernel, and for caching.

Using free
# Display memory usage in human-readable format
free -h

# Sample output:
#               total        used        free      shared  buff/cache   available
# Mem:           31Gi        15Gi       3.5Gi       1.0Gi        13Gi        15Gi
# Swap:         2.0Gi          0B       2.0Gi
Key metrics:

used: Memory used by applications and the kernel
free: Completely unused memory
buff/cache: Memory used for buffers and cache (can be reclaimed if needed)
available: Estimate of memory available for new applications without swapping
Using sar for Memory Utilization Trends
# Display memory utilization every 2 seconds for 10 samples
sar -r 2 10

# View historical memory usage (from sysstat logs)
sar -r -f /var/log/sysstat/sa$(date +%d)

# Check for specific memory usage patterns like "%memused"
sar -r | grep "%memused"
Using top for Process Memory Usage
# Start top
top

# Press Shift+M to sort by memory usage
Memory Saturation
Memory saturation occurs when the system starts swapping or when the OOM (Out of Memory) killer activates.

Monitoring Swap Activity
# Monitor swap usage over time
sar -S 1

# Check if swapping is occurring
vmstat 1 | awk '{print $7 " " $8}'  # si and so columns show swap in/out

# Detailed swap information per process
for file in /proc/*/status; do awk '/VmSwap|Name/{printf $2 " " $3}END{ print ""}' $file 2>/dev/null; done | sort -k 2 -nr | head -20
Checking for OOM Killer Activity
# Search system logs for OOM killer messages
sudo journalctl | grep -i "out of memory"
sudo dmesg | grep -i "oom"

# Monitor kernel OOM messages in real-time
sudo tail -f /var/log/kern.log | grep -i "oom"
Memory Errors
Memory errors can include hardware failures or application memory leaks.

# Check system logs for memory-related errors
sudo journalctl | grep -i "memory\|ram\|dimm"

# Check for memory-related hardware errors
sudo dmesg | grep -i "memory\|ram\|dimm"

# Use memtest86+ for thorough memory testing (requires reboot)
sudo apt install memtest86+
# Reboot and select memtest86+ from the boot menu
Detecting Memory Leaks
# Install valgrind for memory leak detection
sudo apt install valgrind

# Run an application with valgrind
valgrind --leak-check=full --show-leak-kinds=all --track-origins=yes --verbose /path/to/program
Disk Monitoring
Disk Utilization
Disk utilization shows how much of your storage space is being used and the I/O activity.

Using df for Disk Space
# Check disk space usage in human-readable format
df -h

# Check inodes usage (for many small files)
df -i

# Find largest directories
sudo du -h --max-depth=1 /path/to/directory | sort -hr
Using iostat for Disk I/O
# Display disk I/O statistics every 2 seconds
iostat -xz 2

# Sample output:
# Device            r/s     w/s     rkB/s     wkB/s   rrqm/s   wrqm/s  %rrqm  %wrqm r_await w_await aqu-sz rareq-sz wareq-sz  svctm  %util
# sda              0.01    1.21      0.29     33.02     0.00     0.76   0.00  38.47    0.26    2.99   0.00    24.14    27.36   0.11   0.01
# sdb              0.01    0.00      0.29      0.00     0.00     0.00   0.00   0.00    0.31    0.00   0.00    24.14     0.00   0.27   0.00
Key metrics:

r/s, w/s: Reads and writes per second
rkB/s, wkB/s: Read and write kilobytes per second
%util: Percentage of CPU time during which I/O requests were issued
await: Average time for I/O requests (in milliseconds)
Using sar for Disk Utilization Trends
# Display block device statistics every 2 seconds for 10 samples
sar -d 2 10

# View historical disk usage
sar -d -f /var/log/sysstat/sa$(date +%d)
Disk Saturation
Disk saturation occurs when I/O operations are queued, causing delays.

# Monitor disk saturation with iostat
iostat -xz 1 | grep -v "loop"

# Look for high values in the aqu-sz (average queue size) column
Using iotop for Process I/O
# Install iotop if not already installed
sudo apt install iotop

# Monitor disk I/O by process
sudo iotop

# Show only processes doing I/O
sudo iotop -o
Disk Errors
Disk errors can indicate hardware failures or filesystem corruption.

# Check system logs for disk errors
sudo journalctl | grep -i "error\|fail\|bad\|corrupt" | grep -i "disk\|sda\|sdb"

# Check the kernel ring buffer for disk errors
sudo dmesg | grep -i "error\|fail\|bad\|corrupt" | grep -i "disk\|sda\|sdb"
Using SMART Tools for Disk Health
# Install smartmontools if not already installed
sudo apt install smartmontools

# Check disk health
sudo smartctl -a /dev/sda

# Run a short self-test
sudo smartctl -t short /dev/sda

# Check test results
sudo smartctl -l selftest /dev/sda
Network Monitoring
Network Utilization
Network utilization shows how much of your network bandwidth is being used.

Using sar for Network Interface Statistics
# Display network statistics every 2 seconds
sar -n DEV 2

# Sample output:
# 01:23:45 PM     IFACE   rxpck/s   txpck/s    rxkB/s    txkB/s   rxcmp/s   txcmp/s  rxmcst/s
# 01:23:47 PM        lo      0.00      0.00      0.00      0.00      0.00      0.00      0.00
# 01:23:47 PM      eth0    350.50    280.00     45.65     35.60      0.00      0.00      0.00
Key metrics:

rxpck/s, txpck/s: Received and transmitted packets per second
rxkB/s, txkB/s: Received and transmitted kilobytes per second
Using ip or ifconfig for Interface Status
# Display network interface information
ip -s link

# Alternative using ifconfig
ifconfig

# Monitor network traffic in real-time
watch -n 1 "ifconfig eth0 | grep bytes"
Using nethogs for Per-Process Network Usage
# Install nethogs if not already installed
sudo apt install nethogs

# Monitor network traffic by process
sudo nethogs
Network Saturation
Network saturation occurs when the network interface is overwhelmed, leading to dropped packets or retransmissions.

# Check for interface overruns and dropped packets
ifconfig eth0 | grep -E "dropped|overruns"

# Monitor network errors and drops
netstat -i

# Sample output:
# Kernel Interface table
# Iface      MTU    RX-OK RX-ERR RX-DRP RX-OVR    TX-OK TX-ERR TX-DRP TX-OVR Flg
# eth0      1500  3845555      0      0      0  2945555      0      0      0 BMRU
Using netstat for Connection States
# Check TCP connection states
netstat -ant | awk '{print $6}' | sort | uniq -c | sort -n

# Monitor TCP retransmissions
netstat -s | grep -i retrans
Network Errors
Network errors can include packet drops, collisions, or interface failures.

# Check for network errors
netstat -i

# Monitor network errors in real-time
watch -n 1 "netstat -i"

# Check for specific TCP/IP errors
netstat -s | grep -i "error\|drop\|timeout\|fail\|invalid"
Using tcpdump for Packet Analysis
# Install tcpdump if not already installed
sudo apt install tcpdump

# Capture packets on a specific interface
sudo tcpdump -i eth0 -n

# Capture packets for a specific host
sudo tcpdump -i eth0 host 192.168.1.100

# Capture packets for a specific port
sudo tcpdump -i eth0 port 80
Integrated Monitoring Approaches
While individual tools provide specific insights, integrated monitoring approaches offer a more comprehensive view.

Using htop for System Overview
# Install htop if not already installed
sudo apt install htop

# Run htop
htop
htop provides an interactive, real-time view of processes, CPU, and memory usage. Press F2 for setup options to customize the display.

Using atop for System Activity Monitoring
# Install atop if not already installed
sudo apt install atop

# Run atop
atop

# View historical data (if atop service is running)
atop -r /var/log/atop/atop_20230505
atop records system and process activity for later analysis, which is invaluable for troubleshooting intermittent issues.

Using dstat for Combined Resource Statistics
# Install dstat if not already installed
sudo apt install dstat

# Run dstat with combined statistics
dstat -cdnmgs --top-cpu --top-mem

# Custom combination of statistics
dstat -c -d -n -m -g -s 5
dstat combines information from various system resources into a single view, making it easier to spot correlations between different metrics.

Automating Monitoring Tasks
Automation is essential for enterprise-scale monitoring. Here's how to set up basic automated monitoring.

Creating Monitoring Scripts
Create a basic monitoring script:

#!/bin/bash
# File: system_monitor.sh

LOG_FILE="/var/log/system_monitor.log"
DATE=$(date '+%Y-%m-%d %H:%M:%S')

echo "=== System Status at $DATE ===" >> $LOG_FILE

# CPU Load
echo "CPU Load:" >> $LOG_FILE
uptime | awk '{print $10 $11 $12}' >> $LOG_FILE

# Memory Usage
echo "Memory Usage:" >> $LOG_FILE
free -h | grep "Mem:" >> $LOG_FILE

# Disk Usage
echo "Disk Usage:" >> $LOG_FILE
df -h / | grep -v "Filesystem" >> $LOG_FILE

# Network Connections
echo "Network Connections:" >> $LOG_FILE
netstat -ant | wc -l >> $LOG_FILE

# Check for high CPU processes
echo "Top 5 CPU-consuming processes:" >> $LOG_FILE
ps aux --sort=-%cpu | head -6 >> $LOG_FILE

# Check for high memory processes
echo "Top 5 memory-consuming processes:" >> $LOG_FILE
ps aux --sort=-%mem | head -6 >> $LOG_FILE

echo "" >> $LOG_FILE
Make the script executable and set up a cron job:

# Make the script executable
chmod +x system_monitor.sh

# Edit crontab
crontab -e

# Add a line to run the script every 15 minutes
*/15 * * * * /path/to/system_monitor.sh
Setting Up Threshold-Based Alerts
Create a script that sends alerts when thresholds are exceeded:

#!/bin/bash
# File: threshold_alert.sh

# Email settings
RECIPIENT="admin@example.com"
SUBJECT="System Alert"

# Thresholds
CPU_THRESHOLD=90
MEM_THRESHOLD=90
DISK_THRESHOLD=90

# Get current values
CPU_USAGE=$(top -bn1 | grep "Cpu(s)" | awk '{print $2 + $4}')
MEM_USAGE=$(free | grep Mem | awk '{print $3/$2 * 100.0}')
DISK_USAGE=$(df / | tail -1 | awk '{print $5}' | tr -d '%')

# Initialize message
MESSAGE=""

# Check CPU
if (( $(echo "$CPU_USAGE > $CPU_THRESHOLD" | bc -l) )); then
    MESSAGE+="CPU usage is high: ${CPU_USAGE}%\n"
fi

# Check Memory
if (( $(echo "$MEM_USAGE > $MEM_THRESHOLD" | bc -l) )); then
    MESSAGE+="Memory usage is high: ${MEM_USAGE}%\n"
fi

# Check Disk
if [ "$DISK_USAGE" -gt "$DISK_THRESHOLD" ]; then
    MESSAGE+="Disk usage is high: ${DISK_USAGE}%\n"
fi

# Send alert if any thresholds exceeded
if [ ! -z "$MESSAGE" ]; then
    echo -e "$MESSAGE" | mail -s "$SUBJECT" "$RECIPIENT"
fi
Make the script executable and set up a cron job:

# Make the script executable
chmod +x threshold_alert.sh

# Edit crontab
crontab -e

# Add a line to run the script every 5 minutes
*/5 * * * * /path/to/threshold_alert.sh
Implementing Alerting Systems
For enterprise environments, more sophisticated alerting systems are necessary.

Setting Up Prometheus and Alertmanager
Prometheus is a powerful monitoring system with an integrated alert manager:

# Create directories for Prometheus
sudo mkdir -p /etc/prometheus /var/lib/prometheus

# Download Prometheus
wget https://github.com/prometheus/prometheus/releases/download/v2.37.0/prometheus-2.37.0.linux-amd64.tar.gz
tar xvfz prometheus-*.tar.gz
cd prometheus-*

# Copy binaries
sudo cp prometheus promtool /usr/local/bin/
sudo cp -r consoles/ console_libraries/ /etc/prometheus/

# Create a Prometheus configuration file
sudo tee /etc/prometheus/prometheus.yml > /dev/null <<EOF
global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']
  
  - job_name: 'node'
    static_configs:
      - targets: ['localhost:9100']
EOF

# Create a systemd service file
sudo tee /etc/systemd/system/prometheus.service > /dev/null <<EOF
[Unit]
Description=Prometheus
Wants=network-online.target
After=network-online.target

[Service]
User=prometheus
Group=prometheus
Type=simple
ExecStart=/usr/local/bin/prometheus \
    --config.file /etc/prometheus/prometheus.yml \
    --storage.tsdb.path /var/lib/prometheus/ \
    --web.console.templates=/etc/prometheus/consoles \
    --web.console.libraries=/etc/prometheus/console_libraries

[Install]
WantedBy=multi-user.target
EOF

# Create prometheus user
sudo useradd -rs /bin/false prometheus

# Set permissions
sudo chown -R prometheus:prometheus /etc/prometheus /var/lib/prometheus

# Enable and start the service
sudo systemctl enable prometheus
sudo systemctl start prometheus
Installing Node Exporter for System Metrics
Node Exporter collects system metrics for Prometheus:

# Download Node Exporter
wget https://github.com/prometheus/node_exporter/releases/download/v1.3.1/node_exporter-1.3.1.linux-amd64.tar.gz
tar xvfz node_exporter-*.tar.gz
cd node_exporter-*

# Copy binary
sudo cp node_exporter /usr/local/bin/

# Create a systemd service file
sudo tee /etc/systemd/system/node_exporter.service > /dev/null <<EOF
[Unit]
Description=Node Exporter
Wants=network-online.target
After=network-online.target

[Service]
User=node_exporter
Group=node_exporter
Type=simple
ExecStart=/usr/local/bin/node_exporter

[Install]
WantedBy=multi-user.target
EOF

# Create node_exporter user
sudo useradd -rs /bin/false node_exporter

# Enable and start the service
sudo systemctl enable node_exporter
sudo systemctl start node_exporter
Configuring Alertmanager
Set up Alertmanager to handle alerts from Prometheus:

# Download Alertmanager
wget https://github.com/prometheus/alertmanager/releases/download/v0.24.0/alertmanager-0.24.0.linux-amd64.tar.gz
tar xvfz alertmanager-*.tar.gz
cd alertmanager-*

# Copy binaries
sudo cp alertmanager amtool /usr/local/bin/
sudo mkdir -p /etc/alertmanager

# Create a configuration file
sudo tee /etc/alertmanager/alertmanager.yml > /dev/null <<EOF
global:
  smtp_smarthost: 'smtp.example.com:587'
  smtp_from: 'alertmanager@example.com'
  smtp_auth_username: 'alertmanager'
  smtp_auth_password: 'password'

route:
  group_by: ['alertname', 'instance', 'severity']
  group_wait: 30s
  group_interval: 5m
  repeat_interval: 3h
  receiver: 'email'

receivers:
- name: 'email'
  email_configs:
  - to: 'admin@example.com'
EOF

# Create a systemd service file
sudo tee /etc/systemd/system/alertmanager.service > /dev/null <<EOF
[Unit]
Description=Alertmanager
Wants=network-online.target
After=network-online.target

[Service]
User=alertmanager
Group=alertmanager
Type=simple
ExecStart=/usr/local/bin/alertmanager \
    --config.file=/etc/alertmanager/alertmanager.yml \
    --storage.path=/var/lib/alertmanager

[Install]
WantedBy=multi-user.target
EOF

# Create alertmanager user and directory
sudo useradd -rs /bin/false alertmanager
sudo mkdir -p /var/lib/alertmanager
sudo chown alertmanager:alertmanager /var/lib/alertmanager /etc/alertmanager/alertmanager.yml

# Enable and start the service
sudo systemctl enable alertmanager
sudo systemctl start alertmanager
Configuring Prometheus Alert Rules
Create alert rules for Prometheus:

# Create an alert rules file
sudo tee /etc/prometheus/alert.rules.yml > /dev/null <<EOF
groups:
- name: example
  rules:
  - alert: HighCPULoad
    expr: 100 - (avg by(instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "High CPU load (instance {{ \$labels.instance }})"
      description: "CPU load is > 80%\n  VALUE = {{ \$value }}\n  LABELS: {{ \$labels }}"
  
  - alert: HighMemoryLoad
    expr: (node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / node_memory_MemTotal_bytes * 100 > 80
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "High memory load (instance {{ \$labels.instance }})"
      description: "Memory load is > 80%\n  VALUE = {{ \$value }}\n  LABELS: {{ \$labels }}"
  
  - alert: HighDiskUsage
    expr: (node_filesystem_size_bytes{fstype!="tmpfs"} - node_filesystem_free_bytes{fstype!="tmpfs"}) / node_filesystem_size_bytes{fstype!="tmpfs"} * 100 > 80
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "High disk usage (instance {{ \$labels.instance }})"
      description: "Disk usage is > 80%\n  VALUE = {{ \$value }}\n  LABELS: {{ \$labels }}"
EOF

# Update Prometheus configuration to include alert rules
sudo tee /etc/prometheus/prometheus.yml > /dev/null <<EOF
global:
  scrape_interval: 15s

rule_files:
  - "alert.rules.yml"

alerting:
  alertmanagers:
  - static_configs:
    - targets:
      - localhost:9093

scrape_configs:
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']
  
  - job_name: 'node'
    static_configs:
      - targets: ['localhost:9100']
EOF

# Restart Prometheus to apply changes
sudo systemctl restart prometheus
Building Custom Dashboards
Visualizing monitoring data is crucial for effective observability.

Installing and Configuring Grafana
Grafana provides powerful visualization capabilities:

# Add Grafana APT repository
sudo apt-get install -y apt-transport-https software-properties-common
sudo add-apt-repository "deb https://packages.grafana.com/oss/deb stable main"
wget -q -O - https://packages.grafana.com/gpg.key | sudo apt-key add -
sudo apt-get update

# Install Grafana
sudo apt-get install grafana

# Enable and start Grafana
sudo systemctl enable grafana-server
sudo systemctl start grafana-server
Access Grafana at http://your-server-ip:3000 (default credentials: admin/admin)

Creating a Basic System Dashboard
Log in to Grafana
Add Prometheus as a data source:

Go to Configuration > Data Sources > Add data source
Select Prometheus
Set URL to http://localhost:9090
Click "Save & Test"
Import a dashboard:

Go to Create > Import
Enter dashboard ID 1860 (Node Exporter Full)
Select your Prometheus data source
Click "Import"
Creating Custom Dashboard Panels
To create a custom dashboard:

Click "Create" > "Dashboard"
Click "Add new panel"
For CPU usage, use the query:

100 - (avg by (instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)
For memory usage, use:

(node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / node_memory_MemTotal_bytes * 100
For disk usage, use:

(node_filesystem_size_bytes{mountpoint="/"} - node_filesystem_free_bytes{mountpoint="/"}) / node_filesystem_size_bytes{mountpoint="/"} * 100
For network traffic, use:

rate(node_network_receive_bytes_total{device!="lo"}[5m])
and

rate(node_network_transmit_bytes_total{device!="lo"}[5m])
Enterprise-Scale Considerations
For large enterprise environments, additional considerations are necessary.

Centralized Logging with ELK Stack
Set up Elasticsearch, Logstash, and Kibana for centralized logging:

# Install Java
sudo apt update
sudo apt install openjdk-11-jdk

# Add Elasticsearch repository
wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add -
sudo sh -c 'echo "deb https://artifacts.elastic.co/packages/7.x/apt stable main" > /etc/apt/sources.list.d/elastic-7.x.list'
sudo apt update

# Install Elasticsearch
sudo apt install elasticsearch

# Configure Elasticsearch
sudo nano /etc/elasticsearch/elasticsearch.yml
# Set network.host: localhost
# Set cluster.name: monitoring-cluster
# Set node.name: monitoring-node

# Start and enable Elasticsearch
sudo systemctl start elasticsearch
sudo systemctl enable elasticsearch

# Install Kibana
sudo apt install kibana

# Configure Kibana
sudo nano /etc/kibana/kibana.yml
# Set server.port: 5601
# Set server.host: "0.0.0.0"
# Set elasticsearch.hosts: ["http://localhost:9200"]

# Start and enable Kibana
sudo systemctl start kibana
sudo systemctl enable kibana

# Install Logstash
sudo apt install logstash

# Create a basic Logstash configuration
sudo tee /etc/logstash/conf.d/01-syslog-input.conf > /dev/null <<EOF
input {
  syslog {
    port => 5514
    type => "syslog"
  }
}
EOF

sudo tee /etc/logstash/conf.d/30-elasticsearch-output.conf > /dev/null <<EOF
output {
  elasticsearch {
    hosts => ["localhost:9200"]
    index => "logstash-%{+YYYY.MM.dd}"
  }
}
EOF

# Start and enable Logstash
sudo systemctl start logstash
sudo systemctl enable logstash
Configuring rsyslog to Forward Logs
Configure servers to forward logs to the central logging server:

# Edit rsyslog configuration
sudo nano /etc/rsyslog.conf

# Add the following line to forward logs
# Replace logserver.example.com with your log server's address
*.* @logserver.example.com:5514

# Restart rsyslog
sudo systemctl restart rsyslog
Implementing High Availability for Monitoring
For critical environments, set up high availability for monitoring systems:

# Install HAProxy for load balancing
sudo apt install haproxy

# Configure HAProxy
sudo tee /etc/haproxy/haproxy.cfg > /dev/null <<EOF
global
    log /dev/log    local0
    log /dev/log    local1 notice
    chroot /var/lib/haproxy
    stats socket /run/haproxy/admin.sock mode 660 level admin expose-fd listeners
    stats timeout 30s
    user haproxy
    group haproxy
    daemon

defaults
    log     global
    mode    http
    option  httplog
    option  dontlognull
    timeout connect 5000
    timeout client  50000
    timeout server  50000

frontend prometheus_front
    bind *:9090
    default_backend prometheus_back

backend prometheus_back
    balance roundrobin
    server prom1 prometheus1.example.com:9090 check
    server prom2 prometheus2.example.com:9090 check
EOF

# Restart HAProxy
sudo systemctl restart haproxy
Troubleshooting Common Issues
High CPU Usage
When troubleshooting high CPU usage:

# Identify CPU-intensive processes
top -c

# Check for specific process CPU usage
pidstat -p <PID> 1

# Check for system vs user CPU usage
mpstat -P ALL 1

# Check for processes in uninterruptible sleep (I/O wait)
ps aux | awk '$8 ~ /D/'
Memory Leaks
To identify and troubleshoot memory leaks:

# Monitor memory usage over time
watch -n 10 'free -m'

# Check process memory growth
ps -o pid,user,%mem,command ax | sort -b -k3 -r | head -10

# Use pmap to examine process memory maps
pmap -x <PID>

# Check for memory fragmentation
cat /proc/buddyinfo

# Use smem for detailed memory reporting
sudo apt install smem
smem -t -k
Disk I/O Bottlenecks
For disk I/O issues:

# Identify processes causing high I/O
iotop -o

# Check disk I/O statistics
iostat -xz 1

# Check for specific file I/O
sudo apt install iosnoop  # On Ubuntu systems with perf tools
sudo iosnoop -t

# Check for filesystem issues
sudo fsck -N /dev/sda1  # Check without repairing
Network Connectivity Issues
For network troubleshooting:

# Check network connectivity
ping -c 4 8.8.8.8

# Trace network path
traceroute google.com

# Check for network interface errors
ip -s link show eth0

# Monitor network connections
ss -tuln

# Capture network traffic
sudo tcpdump -i eth0 -n port 80
Best Practices
Regular Maintenance Tasks
Implement these regular maintenance tasks:

# Create a maintenance script
sudo tee /usr/local/bin/system-maintenance.sh > /dev/null <<EOF
#!/bin/bash

# Update package lists
apt update

# Check for security updates
apt list --upgradable | grep -i security

# Check disk usage
df -h

# Check for large files
find / -type f -size +100M -exec ls -lh {} \; 2>/dev/null | sort -k5 -hr | head -10

# Check for old logs
find /var/log -type f -name "*.gz" -mtime +30 -exec ls -lh {} \;

# Check for failed services
systemctl --failed

# Check for high CPU processes
ps aux --sort=-%cpu | head -10

# Check for high memory processes
ps aux --sort=-%mem | head -10
EOF

# Make the script executable
sudo chmod +x /usr/local/bin/system-maintenance.sh

# Schedule weekly maintenance
echo "0 2 * * 0 root /usr/local/bin/system-maintenance.sh > /var/log/system-maintenance.log 2>&1" | sudo tee -a /etc/crontab
Security Considerations
Implement security best practices for monitoring systems:

# Secure Prometheus with basic authentication
sudo apt install apache2-utils
sudo htpasswd -c /etc/prometheus/.htpasswd admin

# Update Prometheus configuration
sudo tee /etc/prometheus/web.yml > /dev/null <<EOF
basic_auth_users:
  admin: <hashed-password>
EOF

# Update Prometheus service to use the web config
sudo sed -i 's/ExecStart=.*/ExecStart=\/usr\/local\/bin\/prometheus \\
    --config.file \/etc\/prometheus\/prometheus.yml \\
    --storage.tsdb.path \/var\/lib\/prometheus\/ \\
    --web.console.templates=\/etc\/prometheus\/consoles \\
    --web.console.libraries=\/etc\/prometheus\/console_libraries \\
    --web.config.file=\/etc\/prometheus\/web.yml/' /etc/systemd/system/prometheus.service

# Reload systemd and restart Prometheus
sudo systemctl daemon-reload
sudo systemctl restart prometheus
Documentation and Knowledge Sharing
Create comprehensive documentation for your monitoring setup:

# Create a documentation directory
sudo mkdir -p /opt/documentation/monitoring

# Create a basic documentation file
sudo tee /opt/documentation/monitoring/README.md > /dev/null <<EOF
# System Monitoring Documentation

## Overview
This document describes the monitoring setup for our enterprise Linux environment.

## Components
- Prometheus: Metrics collection and storage
- Node Exporter: System metrics exporter
- Alertmanager: Alert handling and notifications
- Grafana: Visualization and dashboards
- ELK Stack: Log collection and analysis

## Common Tasks
### Checking Prometheus Status
\`\`\`
systemctl status prometheus
\`\`\`

### Viewing Alerts
Access the Prometheus UI at http://localhost:9090/alerts

### Checking Logs
\`\`\`
journalctl -u prometheus
journalctl -u node_exporter
journalctl -u alertmanager
\`\`\`

## Troubleshooting
### Prometheus Not Starting
Check the configuration:
\`\`\`
promtool check config /etc/prometheus/prometheus.yml
\`\`\`

### Missing Metrics
Check if Node Exporter is running:
\`\`\`
systemctl status node_exporter
curl http://localhost:9100/metrics
\`\`\`
EOF
Advanced Topics
Container Monitoring
For monitoring containerized environments:

# Install cAdvisor for container monitoring
sudo docker run \
  --volume=/:/rootfs:ro \
  --volume=/var/run:/var/run:ro \
  --volume=/sys:/sys:ro \
  --volume=/var/lib/docker/:/var/lib/docker:ro \
  --volume=/dev/disk/:/dev/disk:ro \
  --publish=8080:8080 \
  --detach=true \
  --name=cadvisor \
  gcr.io/cadvisor/cadvisor:latest

# Add cAdvisor to Prometheus configuration
sudo tee -a /etc/prometheus/prometheus.yml > /dev/null <<EOF

  - job_name: 'cadvisor'
    static_configs:
      - targets: ['localhost:8080']
EOF

# Restart Prometheus
sudo systemctl restart prometheus
Custom Exporters
Create a custom exporter for application-specific metrics:

# Install required packages
sudo apt install python3-pip
pip3 install prometheus_client

# Create a basic custom exporter
cat > custom_exporter.py <<EOF
#!/usr/bin/env python3

from prometheus_client import start_http_server, Gauge
import random
import time
import subprocess

# Create metrics
cpu_temp = Gauge('cpu_temperature', 'CPU temperature in degrees Celsius')
app_users = Gauge('application_active_users', 'Number of active users in the application')

def get_cpu_temperature():
    # This is a simplified example - adjust for your actual hardware
    try:
        temp = subprocess.check_output(['cat', '/sys/class/thermal/thermal_zone0/temp'])
        return float(temp) / 1000.0
    except:
        return 0

def get_active_users():
    # Replace this with actual logic to get user count from your application
    return random.randint(10, 100)

if __name__ == '__main__':
    # Start up the server to expose the metrics
    start_http_server(8000)
    # Generate some metrics
    while True:
        cpu_temp.set(get_cpu_temperature())
        app_users.set(get_active_users())
        time.sleep(15)
EOF

# Make the script executable
chmod +x custom_exporter.py

# Create a systemd service for the exporter
sudo tee /etc/systemd/system/custom-exporter.service > /dev/null <<EOF
[Unit]
Description=Custom Prometheus Exporter
After=network.target

[Service]
User=prometheus
ExecStart=/usr/bin/python3 /path/to/custom_exporter.py
Restart=always

[Install]
WantedBy=multi-user.target
EOF

# Enable and start the service
sudo systemctl enable custom-exporter
sudo systemctl start custom-exporter

# Add the custom exporter to Prometheus
sudo tee -a /etc/prometheus/prometheus.yml > /dev/null <<EOF

  - job_name: 'custom'
    static_configs:
      - targets: ['localhost:8000']
EOF

# Restart Prometheus
sudo systemctl restart prometheus
Distributed Tracing
Implement distributed tracing for complex applications:

# Install Jaeger for distributed tracing
docker run -d --name jaeger \
  -e COLLECTOR_ZIPKIN_HOST_PORT=:9411 \
  -p 5775:5775/udp \
  -p 6831:6831/udp \
  -p 6832:6832/udp \
  -p 5778:5778 \
  -p 16686:16686 \
  -p 14268:14268 \
  -p 14250:14250 \
  -p 9411:9411 \
  jaegertracing/all-in-one:latest

# Access the Jaeger UI at http://localhost:16686
Conclusion
Effective system observability is crucial for maintaining reliable enterprise Linux environments. By systematically monitoring the "Big 4" resources—CPU, Memory, Disk, and Network—across the dimensions of utilization, saturation, and errors, you can proactively identify and resolve issues before they impact users.

This guide has covered:

Essential monitoring tools for each resource
Setting up automated monitoring and alerting
Implementing enterprise-scale monitoring solutions
Troubleshooting common issues
Best practices for maintaining your monitoring infrastructure
Remember that observability is not just about collecting data but about gaining actionable insights. Regularly review your monitoring setup to ensure it continues to meet your organization's needs as your infrastructure evolves.

By following the practices outlined in this guide, you'll be well-equipped to maintain high-performance, reliable Linux systems in your enterprise environment.

Chat Input


No file chosen
v0 may make mistakes
